# 3ë‹¨ê³„: RL/ML ê³ ë„í™” êµ¬í˜„ ê³„íšì„œ

> ì‘ì„±ì¼: 2025-12-22
> ë²„ì „: v1.1
> ìƒíƒœ: âœ… êµ¬í˜„ ì™„ë£Œ

---

## âœ… êµ¬í˜„ ì™„ë£Œ ìš”ì•½

**êµ¬í˜„ ì¼ì‹œ**: 2025-12-22 14:10

**êµ¬í˜„ëœ ê¸°ëŠ¥**:
- Dueling Double DQN (D3QN) ì‹ ê²½ë§
- ì—°ì† ìƒíƒœ ì¸ì½”ë” (12ì°¨ì› Ã— 20ì¼ ìœˆë„ìš°)
- ê²½í—˜ ì¬í˜„ ë²„í¼ (Uniform + Prioritized)
- ë‹¤ë©´ì  ë³´ìƒ í•¨ìˆ˜ (Risk-Adjusted)
- ê¸°ì¡´ QLearnerì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**:
- âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ
- âœ… RewardShaper ì •ìƒ ì‘ë™ (ì¢‹ì€ ê²°ê³¼: 0.536, ë‚˜ìœ ê²°ê³¼: -0.364)
- âœ… MetaAgent D3QN ëª¨ë“œ ì •ìƒ ì‘ë™
- âœ… í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“‹ ëª©ì°¨

1. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„](#1-í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„)
2. [ëª©í‘œ ì•„í‚¤í…ì²˜](#2-ëª©í‘œ-ì•„í‚¤í…ì²˜)
3. [êµ¬í˜„ ìƒì„¸](#3-êµ¬í˜„-ìƒì„¸)
4. [íŒŒì¼ ë³€ê²½ ê³„íš](#4-íŒŒì¼-ë³€ê²½-ê³„íš)
5. [í…ŒìŠ¤íŠ¸ ê³„íš](#5-í…ŒìŠ¤íŠ¸-ê³„íš)
6. [ë¡¤ë°± ì „ëµ](#6-ë¡¤ë°±-ì „ëµ)
7. [ì¼ì •](#7-ì¼ì •)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] Configì— D3QN ì„¤ì • ì¶”ê°€
- [x] StateEncoder êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [x] ReplayBuffer êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [x] RewardShaper êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [x] DuelingDQN ì‹ ê²½ë§ êµ¬í˜„
- [x] D3QNAgent êµ¬í˜„ ë° QLearner ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜
- [x] MetaAgent í†µí•©
- [x] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [x] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í™•ì¸

---

## 1. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### 1.1 í˜„ì¬ RL êµ¬ì¡° (Tabular Q-Learning)

```
í˜„ì¬ ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RegimeDetector â”‚â”€â”€â”€â”€â–¶â”‚   QLearner   â”‚â”€â”€â”€â”€â–¶â”‚  MetaAgent  â”‚
â”‚  (ë¼ë²¨ ë¶„ë¥˜)     â”‚     â”‚  (Q-Table)   â”‚     â”‚ (ì •ì±… ìƒì„±)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                    â”‚
        â–¼                      â–¼                    â–¼
   "PANIC" ë“± 7ê°œ       Dict[str, List[float]]   PolicySpec
   ì´ì‚° ë¼ë²¨             ìƒíƒœ-í–‰ë™ í…Œì´ë¸”
```

### 1.2 í˜„ì¬ ë¬¸ì œì 

| ë¬¸ì œ | í˜„ì¬ ìƒíƒœ | ì˜í–¥ |
|------|-----------|------|
| **ìƒíƒœ í‘œí˜„ ë‹¨ìˆœí™”** | 7ê°œ ì´ì‚° ë¼ë²¨ë§Œ ì‚¬ìš© (PANIC, GOLDILOCKS ë“±) | ì„¸ë°€í•œ ì‹œì¥ ìƒí™© êµ¬ë¶„ ë¶ˆê°€ |
| **Q-Table í•œê³„** | ìƒíƒœ-í–‰ë™ ìŒì„ í…Œì´ë¸”ë¡œ ì €ì¥ | ìƒˆë¡œìš´ ìƒíƒœì— ëŒ€í•œ ì¼ë°˜í™” ë¶ˆê°€ |
| **ë³´ìƒ ë‹¨ìˆœí™”** | í‰ê°€ ìŠ¤ì½”ì–´ë§Œ ë³´ìƒìœ¼ë¡œ ì‚¬ìš© | ìœ„í—˜ ì¡°ì • ìˆ˜ìµë¥  ë¯¸ë°˜ì˜ |
| **exec() ì‚¬ìš©** | í”¼ì²˜ ì½”ë“œë¥¼ ë™ì  ì‹¤í–‰ | ë³´ì•ˆ/ë””ë²„ê¹… ì–´ë ¤ì›€ |
| **ì‹œê³„ì—´ ë¬´ì‹œ** | í˜„ì¬ ì‹œì ë§Œ ê³ ë ¤ | ì¶”ì„¸/ëª¨ë©˜í…€ ì •ë³´ ì†ì‹¤ |

### 1.3 í˜„ì¬ íŒŒì¼ êµ¬ì¡°

```
src/l3_meta/
â”œâ”€â”€ agent.py           # MetaAgent (ì •ì±… ìƒì„±)
â”œâ”€â”€ q_learner.py       # Tabular Q-Learning â¬…ï¸ ëŒ€ì²´ ëŒ€ìƒ
â”œâ”€â”€ state.py           # RegimeState (ìƒíƒœ ì •ì˜)
â”œâ”€â”€ risk_profiles.py   # ë¦¬ìŠ¤í¬ í”„ë¡œíŒŒì¼
â”œâ”€â”€ detectors/
â”‚   â””â”€â”€ regime.py      # RegimeDetector (ì‹œì¥ ìƒíƒœ ë¶„ë¥˜)
â””â”€â”€ ...
```

---

## 2. ëª©í‘œ ì•„í‚¤í…ì²˜

### 2.1 ìƒˆë¡œìš´ RL êµ¬ì¡° (Dueling Double DQN)

```
ëª©í‘œ ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  StateEncoder   â”‚â”€â”€â”€â”€â–¶â”‚      D3QN        â”‚â”€â”€â”€â”€â–¶â”‚   MetaAgent     â”‚
â”‚  (ì—°ì† ìƒíƒœ)     â”‚     â”‚  (ì‹ ê²½ë§ ê¸°ë°˜)    â”‚     â”‚  (ì •ì±… ìƒì„±)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                       â”‚
        â–¼                        â–¼                       â–¼
  [VIX, Trend, Vol,      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          PolicySpec
   Returns, ...]         â”‚   V(s)       â”‚   (Value Stream)
   ì‹¤ìˆ˜ ë²¡í„°             â”‚   A(s,a)     â”‚   (Advantage Stream)
   + ì‹œê³„ì—´ ìœˆë„ìš°       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ ReplayBuffer â”‚
                        â”‚ (ê²½í—˜ ì¬í˜„)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 í•µì‹¬ ê°œì„  ì‚¬í•­

| ì˜ì—­ | ì´ì „ | ì´í›„ |
|------|------|------|
| **ìƒíƒœ í‘œí˜„** | ì´ì‚° ë¼ë²¨ (7ê°œ) | ì—°ì† ë²¡í„° (Nì°¨ì›) + ì‹œê³„ì—´ ìœˆë„ìš° |
| **í•™ìŠµ ì•Œê³ ë¦¬ì¦˜** | Tabular Q-Learning | Dueling Double DQN (D3QN) |
| **ë³´ìƒ í•¨ìˆ˜** | ë‹¨ìˆœ ìŠ¤ì½”ì–´ | Risk-Adjusted Multi-Factor Reward |
| **ê²½í—˜ ì¬í˜„** | ì—†ìŒ | Prioritized Experience Replay |
| **í•˜ë“œì›¨ì–´** | CPU only | CPU + GPU (ì„ íƒì ) |

---

## 3. êµ¬í˜„ ìƒì„¸

### 3.1 ìƒíƒœ ì¸ì½”ë” (StateEncoder)

**ëª©ì **: ì—°ì†ì ì¸ ì‹œì¥ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ ì…ë ¥ìœ¼ë¡œ ë³€í™˜

**íŒŒì¼**: `src/l3_meta/state_encoder.py` (ì‹ ê·œ)

```python
# ì˜ì‚¬ ì½”ë“œ
class StateEncoder:
    """
    ì‹œì¥ ìƒíƒœë¥¼ ì‹ ê²½ë§ ì…ë ¥ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    ì…ë ¥ íŠ¹ì„± (12ì°¨ì›):
    - VIX (ë³€ë™ì„± ì§€ìˆ˜)
    - VIX ë³€í™”ìœ¨ (5ì¼)
    - ì¶”ì„¸ ì ìˆ˜ (ADX ê¸°ë°˜)
    - ìƒê´€ê´€ê³„ ì ìˆ˜ (SPY vs QQQ)
    - ìµœê·¼ ìˆ˜ìµë¥  (5ì¼, 20ì¼)
    - ëª¨ë©˜í…€ (RSI ì •ê·œí™”)
    - ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
    - ê±°ë˜ëŸ‰ ë¹„ìœ¨
    - ê¸ˆë¦¬ ìŠ¤í”„ë ˆë“œ (10Y-2Y)
    - ë‹¬ëŸ¬ ì§€ìˆ˜ ë³€í™”ìœ¨
    - ìµœê·¼ ë³€ë™ì„± (ì‹¤í˜„ ë³€ë™ì„±)
    - ì‹œì¥ êµ­ë©´ ì ìˆ˜ (ì—°ì†ê°’)
    
    ì‹œê³„ì—´ ìœˆë„ìš°:
    - ìµœê·¼ Nì¼ (ê¸°ë³¸ 20ì¼)ì˜ ìƒíƒœ ë²¡í„°ë¥¼ ìŠ¤íƒ
    - Shape: (window_size, feature_dim) = (20, 12)
    """
    
    def __init__(self, window_size: int = 20, feature_dim: int = 12):
        self.window_size = window_size
        self.feature_dim = feature_dim
        self.scaler = None  # ì •ê·œí™”ê¸° (í•™ìŠµ í›„ ì €ì¥)
        
    def encode(self, df: pd.DataFrame) -> np.ndarray:
        """
        DataFrameì„ ìƒíƒœ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            np.ndarray: Shape (window_size, feature_dim)
        """
        pass
    
    def get_state_dim(self) -> int:
        """ìƒíƒœ ë²¡í„°ì˜ ì´ ì°¨ì› ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.window_size * self.feature_dim
```

### 3.2 D3QN ì‹ ê²½ë§ (DuelingDQN)

**ëª©ì **: ìƒíƒœì—ì„œ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ì‹¬ì¸µ ì‹ ê²½ë§

**íŒŒì¼**: `src/l3_meta/d3qn.py` (ì‹ ê·œ)

```python
# ì˜ì‚¬ ì½”ë“œ
class DuelingDQN(nn.Module):
    """
    Dueling Double DQN ì‹ ê²½ë§.
    
    êµ¬ì¡°:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Input     â”‚  (window_size * feature_dim)
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Shared FC  â”‚  Linear(state_dim, 256) + ReLU
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â–¼             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Value   â”‚  â”‚Advantageâ”‚
    â”‚ Stream  â”‚  â”‚ Stream  â”‚
    â”‚ FC(128) â”‚  â”‚ FC(128) â”‚
    â”‚ FC(1)   â”‚  â”‚ FC(n_a) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚            â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚  Combine    â”‚  Q(s,a) = V(s) + (A(s,a) - mean(A))
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚  Output     â”‚  (n_actions)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    def __init__(self, state_dim: int, n_actions: int, hidden_dim: int = 256):
        super().__init__()
        
        # ê³µìœ  ë ˆì´ì–´
        self.shared = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        
        # Value Stream (ìƒíƒœ ê°€ì¹˜)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
        )
        
        # Advantage Stream (í–‰ë™ ì´ì )
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, n_actions),
        )
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))
        """
        shared = self.shared(state)
        value = self.value_stream(shared)
        advantage = self.advantage_stream(shared)
        
        # Dueling ê²°í•© ê³µì‹
        q_values = value + (advantage - advantage.mean(dim=-1, keepdim=True))
        return q_values
```

### 3.3 ê²½í—˜ ì¬í˜„ ë²„í¼ (ReplayBuffer)

**ëª©ì **: ê³¼ê±° ê²½í—˜ì„ ì €ì¥í•˜ê³  ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

**íŒŒì¼**: `src/l3_meta/replay_buffer.py` (ì‹ ê·œ)

```python
# ì˜ì‚¬ ì½”ë“œ
@dataclass
class Experience:
    """ë‹¨ì¼ ê²½í—˜ (SARS')"""
    state: np.ndarray       # í˜„ì¬ ìƒíƒœ
    action: int             # ì„ íƒí•œ í–‰ë™
    reward: float           # ë°›ì€ ë³´ìƒ
    next_state: np.ndarray  # ë‹¤ìŒ ìƒíƒœ
    done: bool              # ì¢…ë£Œ ì—¬ë¶€


class ReplayBuffer:
    """
    ê²½í—˜ ì¬í˜„ ë²„í¼ (Circular Buffer).
    
    ê¸°ëŠ¥:
    - ìµœê·¼ Nê°œ ê²½í—˜ ì €ì¥
    - ëœë¤ ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§
    - ì„ íƒì : ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìƒ˜í”Œë§ (PER)
    """
    
    def __init__(self, capacity: int = 10000, batch_size: int = 64):
        self.capacity = capacity
        self.batch_size = batch_size
        self.buffer = deque(maxlen=capacity)
    
    def push(self, experience: Experience) -> None:
        """ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        self.buffer.append(experience)
    
    def sample(self) -> List[Experience]:
        """ëœë¤í•˜ê²Œ ë°°ì¹˜ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."""
        return random.sample(self.buffer, min(len(self.buffer), self.batch_size))
    
    def __len__(self) -> int:
        return len(self.buffer)
```

### 3.4 ë³´ìƒ ì—”ì§„ (RewardShaper)

**ëª©ì **: ë‹¤ë©´ì  ë³´ìƒ í•¨ìˆ˜ë¡œ ì—ì´ì „íŠ¸ í•™ìŠµ ê°€ì´ë“œ

**íŒŒì¼**: `src/l3_meta/reward_shaper.py` (ì‹ ê·œ)

```python
# ì˜ì‚¬ ì½”ë“œ
class RewardShaper:
    """
    ë‹¤ë©´ì  ë³´ìƒ í•¨ìˆ˜.
    
    ë³´ìƒ êµ¬ì„±:
    R = w_return * R_return      # ìˆ˜ìµë¥  ë³´ìƒ
      + w_sharpe * R_sharpe      # ìƒ¤í”„ ë¹„ìœ¨ ë³´ìƒ
      + w_mdd * R_mdd            # MDD í˜ë„í‹°
      + w_trades * R_trades      # ê±°ë˜ íš¨ìœ¨ ë³´ìƒ
      + w_stability * R_stability # ì•ˆì •ì„± ë³´ìƒ
    
    ê° ë³´ìƒ ë²”ìœ„: [-1, 1] ì •ê·œí™”
    """
    
    # ë³´ìƒ ê°€ì¤‘ì¹˜ (configì—ì„œ ë¡œë“œ)
    WEIGHTS = {
        "return": 0.3,       # ìˆ˜ìµë¥ 
        "sharpe": 0.25,      # ìœ„í—˜ ì¡°ì • ìˆ˜ìµ
        "mdd": 0.2,          # ìµœëŒ€ ë‚™í­ í˜ë„í‹°
        "trades": 0.15,      # ê±°ë˜ íš¨ìœ¨
        "stability": 0.1,    # ìˆ˜ìµ ì•ˆì •ì„±
    }
    
    def compute(self, metrics: Dict[str, float]) -> float:
        """
        í‰ê°€ ì§€í‘œì—ì„œ ë³µí•© ë³´ìƒì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            metrics: {
                "total_return": float,  # ì´ ìˆ˜ìµë¥  (%)
                "sharpe": float,        # ìƒ¤í”„ ë¹„ìœ¨
                "mdd": float,           # ìµœëŒ€ ë‚™í­ (%)
                "n_trades": int,        # ê±°ë˜ íšŸìˆ˜
                "win_rate": float,      # ìŠ¹ë¥ 
                "cpcv_std": float,      # ìˆ˜ìµ ë³€ë™ì„±
            }
        
        Returns:
            float: ë³µí•© ë³´ìƒ (ëŒ€ëµ [-1, 1] ë²”ìœ„)
        """
        pass
    
    def _normalize(self, value: float, min_val: float, max_val: float) -> float:
        """ê°’ì„ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        pass
```

### 3.5 D3QN ì—ì´ì „íŠ¸ (D3QNAgent)

**ëª©ì **: ê¸°ì¡´ QLearnerë¥¼ ëŒ€ì²´í•˜ëŠ” Deep RL ì—ì´ì „íŠ¸

**íŒŒì¼**: `src/l3_meta/d3qn_agent.py` (ì‹ ê·œ)

```python
# ì˜ì‚¬ ì½”ë“œ
class D3QNAgent:
    """
    Dueling Double DQN ì—ì´ì „íŠ¸.
    
    ê¸°ì¡´ QLearnerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ í˜¸í™˜ì„± ìœ ì§€.
    
    ì£¼ìš” ë©”ì„œë“œ:
    - get_action(regime) -> (action_name, action_idx)
    - update(reward, next_regime, ...)
    - save() / load()
    
    Double DQN ë¡œì§:
    - Online Network: í–‰ë™ ì„ íƒì— ì‚¬ìš©
    - Target Network: Q ê°’ í‰ê°€ì— ì‚¬ìš©
    - ì£¼ê¸°ì ìœ¼ë¡œ Targetì„ Onlineìœ¼ë¡œ ë³µì‚¬ (Soft Update)
    """
    
    def __init__(self, storage_path: Path, actions: List[str] = None):
        self.actions = actions or DEFAULT_ACTIONS
        self.n_actions = len(self.actions)
        
        # ìƒíƒœ ì¸ì½”ë”
        self.state_encoder = StateEncoder()
        
        # ì‹ ê²½ë§ (Online & Target)
        state_dim = self.state_encoder.get_state_dim()
        self.online_net = DuelingDQN(state_dim, self.n_actions)
        self.target_net = DuelingDQN(state_dim, self.n_actions)
        self.target_net.load_state_dict(self.online_net.state_dict())
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=1e-4)
        
        # ê²½í—˜ ì¬í˜„ ë²„í¼
        self.replay_buffer = ReplayBuffer(capacity=10000, batch_size=64)
        
        # ë³´ìƒ ê³„ì‚°ê¸°
        self.reward_shaper = RewardShaper()
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma = 0.99
        self.tau = 0.005  # Soft update ë¹„ìœ¨
        self.epsilon = 0.2
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05
        self.update_freq = 4  # í•™ìŠµ ë¹ˆë„
        
        # ìƒíƒœ ì¶”ì 
        self.last_state = None
        self.last_action = None
        self.step_count = 0
    
    def get_action(self, regime: RegimeState, df: pd.DataFrame = None) -> Tuple[str, int]:
        """
        í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤ (epsilon-greedy).
        """
        # ìƒíƒœ ì¸ì½”ë”©
        state = self.state_encoder.encode(df) if df is not None else self._regime_to_vector(regime)
        
        # Epsilon-greedy
        if random.random() < self.epsilon:
            action_idx = random.randint(0, self.n_actions - 1)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.online_net(state_tensor)
                action_idx = q_values.argmax(dim=-1).item()
        
        self.last_state = state
        self.last_action = action_idx
        
        return self.actions[action_idx], action_idx
    
    def update(self, reward: float, next_regime: RegimeState, **kwargs):
        """
        ê²½í—˜ì„ ì €ì¥í•˜ê³  ì‹ ê²½ë§ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        """
        # 1. ê²½í—˜ ì €ì¥
        next_state = kwargs.get('next_state', self._regime_to_vector(next_regime))
        experience = Experience(
            state=self.last_state,
            action=self.last_action,
            reward=reward,
            next_state=next_state,
            done=False,
        )
        self.replay_buffer.push(experience)
        
        # 2. í•™ìŠµ (ì¼ì • ë¹ˆë„ë¡œ)
        self.step_count += 1
        if self.step_count % self.update_freq == 0 and len(self.replay_buffer) >= 64:
            self._learn()
        
        # 3. Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def _learn(self):
        """
        ê²½í—˜ ì¬í˜„ ë²„í¼ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
        
        Double DQN Loss:
        - action_select = argmax(Q_online(s'))
        - Q_target = r + gamma * Q_target(s', action_select)
        - Loss = MSE(Q_online(s, a), Q_target)
        """
        batch = self.replay_buffer.sample()
        
        # ë°°ì¹˜ í…ì„œ ë³€í™˜
        states = torch.FloatTensor([e.state for e in batch])
        actions = torch.LongTensor([e.action for e in batch])
        rewards = torch.FloatTensor([e.reward for e in batch])
        next_states = torch.FloatTensor([e.next_state for e in batch])
        dones = torch.FloatTensor([e.done for e in batch])
        
        # Double DQN: Onlineìœ¼ë¡œ í–‰ë™ ì„ íƒ, Targetìœ¼ë¡œ ê°€ì¹˜ í‰ê°€
        with torch.no_grad():
            next_actions = self.online_net(next_states).argmax(dim=-1)
            next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(-1)).squeeze(-1)
            target_q = rewards + self.gamma * next_q_values * (1 - dones)
        
        # Current Q
        current_q = self.online_net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)
        
        # Loss & Backprop
        loss = F.mse_loss(current_q, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), 1.0)
        self.optimizer.step()
        
        # Soft Update Target Network
        self._soft_update()
    
    def _soft_update(self):
        """Target ë„¤íŠ¸ì›Œí¬ë¥¼ Soft Updateí•©ë‹ˆë‹¤."""
        for target_param, online_param in zip(
            self.target_net.parameters(), 
            self.online_net.parameters()
        ):
            target_param.data.copy_(
                self.tau * online_param.data + (1 - self.tau) * target_param.data
            )
```

---

## 4. íŒŒì¼ ë³€ê²½ ê³„íš

### 4.1 ì‹ ê·œ íŒŒì¼

| íŒŒì¼ | ì„¤ëª… | ìš°ì„ ìˆœìœ„ |
|------|------|----------|
| `src/l3_meta/state_encoder.py` | ìƒíƒœ ì¸ì½”ë” | ğŸ”´ P0 |
| `src/l3_meta/d3qn.py` | Dueling DQN ì‹ ê²½ë§ | ğŸ”´ P0 |
| `src/l3_meta/replay_buffer.py` | ê²½í—˜ ì¬í˜„ ë²„í¼ | ğŸ”´ P0 |
| `src/l3_meta/reward_shaper.py` | ë³´ìƒ ì—”ì§„ | ğŸ”´ P0 |
| `src/l3_meta/d3qn_agent.py` | D3QN ì—ì´ì „íŠ¸ | ğŸ”´ P0 |
| `tests/unit/test_d3qn.py` | ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | ğŸŸ¡ P1 |

### 4.2 ìˆ˜ì • íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© | ìš°ì„ ìˆœìœ„ |
|------|-----------|----------|
| `src/config.py` | D3QN í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ê°€ | ğŸ”´ P0 |
| `src/l3_meta/agent.py` | D3QNAgent ì‚¬ìš© ì˜µì…˜ ì¶”ê°€ | ğŸ”´ P0 |
| `src/l3_meta/state.py` | EncodedState ì¶”ê°€ | ğŸŸ¡ P1 |
| `src/orchestration/infinite_loop.py` | dfë¥¼ ì—ì´ì „íŠ¸ì— ì „ë‹¬ | ğŸŸ¡ P1 |

### 4.3 Config ì¶”ê°€ í•­ëª©

```python
# D3QN ì„¤ì •
D3QN_ENABLED: bool = True                    # D3QN ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ê¸°ì¡´ Q-Table)
D3QN_HIDDEN_DIM: int = 256                   # ì€ë‹‰ì¸µ ì°¨ì›
D3QN_LEARNING_RATE: float = 1e-4             # í•™ìŠµë¥ 
D3QN_GAMMA: float = 0.99                     # í• ì¸ìœ¨
D3QN_TAU: float = 0.005                      # Soft update ë¹„ìœ¨
D3QN_BUFFER_SIZE: int = 10000                # ë²„í¼ í¬ê¸°
D3QN_BATCH_SIZE: int = 64                    # ë°°ì¹˜ í¬ê¸°
D3QN_UPDATE_FREQ: int = 4                    # í•™ìŠµ ë¹ˆë„

# ìƒíƒœ ì¸ì½”ë” ì„¤ì •
STATE_WINDOW_SIZE: int = 20                  # ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
STATE_FEATURE_DIM: int = 12                  # íŠ¹ì„± ì°¨ì›

# ë³´ìƒ ê°€ì¤‘ì¹˜
REWARD_W_RETURN: float = 0.30                # ìˆ˜ìµë¥  ê°€ì¤‘ì¹˜
REWARD_W_SHARPE: float = 0.25                # ìƒ¤í”„ ê°€ì¤‘ì¹˜
REWARD_W_MDD: float = 0.20                   # MDD ê°€ì¤‘ì¹˜
REWARD_W_TRADES: float = 0.15                # ê±°ë˜ íš¨ìœ¨ ê°€ì¤‘ì¹˜
REWARD_W_STABILITY: float = 0.10             # ì•ˆì •ì„± ê°€ì¤‘ì¹˜
REWARD_MDD_THRESHOLD: float = 15.0           # MDD í˜ë„í‹° ì„ê³„ê°’ (%)
```

---

## 5. í…ŒìŠ¤íŠ¸ ê³„íš

### 5.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

| í…ŒìŠ¤íŠ¸ | ê²€ì¦ í•­ëª© |
|--------|-----------|
| `test_state_encoder.py` | ì…ë ¥/ì¶œë ¥ ì°¨ì›, ì •ê·œí™”, ìœˆë„ìš° ì²˜ë¦¬ |
| `test_d3qn_network.py` | Forward pass, ì¶œë ¥ ì°¨ì›, Value/Advantage ë¶„ë¦¬ |
| `test_replay_buffer.py` | Push/Sample, ìš©ëŸ‰ ì œí•œ, ë°°ì¹˜ ìƒ˜í”Œë§ |
| `test_reward_shaper.py` | ë³´ìƒ ë²”ìœ„, ê°€ì¤‘ì¹˜ ì ìš©, ì •ê·œí™” |
| `test_d3qn_agent.py` | í–‰ë™ ì„ íƒ, í•™ìŠµ ë£¨í”„, ì €ì¥/ë¡œë“œ |

### 5.2 í†µí•© í…ŒìŠ¤íŠ¸

| í…ŒìŠ¤íŠ¸ | ê²€ì¦ í•­ëª© |
|--------|-----------|
| `test_d3qn_integration.py` | ì „ì²´ íŒŒì´í”„ë¼ì¸ (ìƒíƒœâ†’í–‰ë™â†’ë³´ìƒâ†’í•™ìŠµ) |
| `test_backward_compatibility.py` | ê¸°ì¡´ QLearnerì™€ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„± |

### 5.3 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ì§€í‘œ | ëª©í‘œ |
|------|------|
| í•™ìŠµ ìˆ˜ë ´ ì†ë„ | ê¸°ì¡´ ëŒ€ë¹„ 2ë°° í–¥ìƒ |
| ìµœì¢… ë³´ìƒ | ê¸°ì¡´ ëŒ€ë¹„ 20% í–¥ìƒ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | < 2GB |
| GPU ì‚¬ìš©ëŸ‰ (ì„ íƒì ) | < 50% |

---

## 6. ë¡¤ë°± ì „ëµ

### 6.1 Feature Flag

```python
# config.py
D3QN_ENABLED: bool = True  # Falseë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ QLearner ì‚¬ìš©
```

### 6.2 ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„±

```python
# agent.pyì—ì„œ
if config.D3QN_ENABLED:
    self.strategy_rl = D3QNAgent(repo.base_dir)
else:
    self.strategy_rl = QLearner(repo.base_dir)  # ê¸°ì¡´ ë°©ì‹
```

### 6.3 ì €ì¥ëœ ëª¨ë¸ ë§ˆì´ê·¸ë ˆì´ì…˜

```python
# ê¸°ì¡´ Q-Tableì´ ìˆìœ¼ë©´ D3QN ì´ˆê¸°í™”ì— í™œìš©
def migrate_q_table_to_d3qn(q_table_path: Path, d3qn_agent: D3QNAgent):
    """Q-Tableì˜ ì§€ì‹ì„ D3QN ì‚¬ì „ í•™ìŠµì— í™œìš©í•©ë‹ˆë‹¤."""
    pass
```

---

## 7. ì¼ì •

### Phase 3.1: ê¸°ë°˜ êµ¬ì¡° (ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„)

1. âœ… Configì— D3QN ì„¤ì • ì¶”ê°€
2. âœ… StateEncoder êµ¬í˜„
3. âœ… ReplayBuffer êµ¬í˜„
4. âœ… RewardShaper êµ¬í˜„

### Phase 3.2: ì‹ ê²½ë§ êµ¬í˜„ (ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„)

1. âœ… DuelingDQN ì‹ ê²½ë§ êµ¬í˜„
2. âœ… D3QNAgent êµ¬í˜„
3. âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜ì„± í™•ë³´

### Phase 3.3: í†µí•© (ì˜ˆìƒ ì†Œìš”: 30ë¶„)

1. âœ… MetaAgentì— D3QN ì˜µì…˜ í†µí•©
2. âœ… infinite_loopì— df ì „ë‹¬ ì¶”ê°€
3. âœ… Feature Flag ë™ì‘ í™•ì¸

### Phase 3.4: í…ŒìŠ¤íŠ¸ (ì˜ˆìƒ ì†Œìš”: 30ë¶„)

1. âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
2. âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
3. âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

---

## ğŸ“ ì˜ì¡´ì„±

### í•„ìˆ˜ íŒ¨í‚¤ì§€

```txt
torch>=2.0.0           # PyTorch (D3QN ì‹ ê²½ë§)
numpy>=1.24.0          # ìˆ˜ì¹˜ ì—°ì‚°
pandas>=2.0.0          # ë°ì´í„° ì²˜ë¦¬ (ì´ë¯¸ ì„¤ì¹˜ë¨)
```

### ì„ íƒ íŒ¨í‚¤ì§€

```txt
tensorboard>=2.15.0    # í•™ìŠµ ì‹œê°í™” (ì„ íƒ)
```

---

## ğŸ” ì°¸ê³  ìë£Œ

- [Dueling DQN ë…¼ë¬¸](https://arxiv.org/abs/1511.06581)
- [Double DQN ë…¼ë¬¸](https://arxiv.org/abs/1509.06461)
- [ê¸ˆìœµ RL ì ìš© ì‚¬ë¡€](https://arxiv.org/abs/2111.05188)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Configì— D3QN ì„¤ì • ì¶”ê°€
- [ ] StateEncoder êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] ReplayBuffer êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] RewardShaper êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸
- [ ] DuelingDQN ì‹ ê²½ë§ êµ¬í˜„
- [ ] D3QNAgent êµ¬í˜„ ë° QLearner ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜
- [ ] MetaAgent í†µí•©
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í™•ì¸
