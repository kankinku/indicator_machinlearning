"""
D3QN Agent - Dueling Double DQN ì—ì´ì „íŠ¸

ê¸°ì¡´ QLearnerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•˜ì—¬ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
ë‚´ë¶€ì ìœ¼ë¡œëŠ” ì‹ ê²½ë§ ê¸°ë°˜ Deep RLì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- ì—°ì† ìƒíƒœ ê³µê°„ ì²˜ë¦¬
- ê²½í—˜ ì¬í˜„ì„ í†µí•œ ì•ˆì •ì  í•™ìŠµ
- Double DQNìœ¼ë¡œ Qê°’ ê³¼ëŒ€í‰ê°€ ë°©ì§€
- Soft Updateë¡œ Target ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
"""
from __future__ import annotations

import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import numpy as np

from src.config import config
from src.shared.logger import get_logger
from src.l3_meta.state import RegimeState
from src.l3_meta.state_encoder import StateEncoder, get_state_encoder
from src.l3_meta.replay_buffer import ReplayBuffer, Experience, create_replay_buffer
from src.l3_meta.reward_shaper import RewardShaper, get_reward_shaper
from src.l3_meta.d3qn import (
    DuelingDQN, 
    MultiHeadDuelingDQN,
    create_dqn_pair, 
    soft_update, 
    TORCH_AVAILABLE,
    get_device,
)

logger = get_logger("l3.d3qn_agent")

# PyTorch ì¡°ê±´ë¶€ ì„í¬íŠ¸
if TORCH_AVAILABLE:
    import torch
    import torch.nn.functional as F


# ê¸°ë³¸ í–‰ë™ ê³µê°„ (QLearnerì™€ ë™ì¼)
DEFAULT_ACTIONS = [
    "TREND_FOLLOWING",  # MA, MACD, Parabolic SAR
    "MEAN_REVERSION",   # RSI, Bollinger, Stochastic
    "VOLATILITY_BREAK", # ATR, Keltner, Bands
    "MOMENTUM_ALPHA",   # ROC, CCI, AO
    "DIP_BUYING",       # Trend Long + RSI Oversold
    "DEFENSIVE"         # Strict risk, slow MAs
]


class D3QNAgent:
    """
    Dueling Double DQN ì—ì´ì „íŠ¸.
    
    ê¸°ì¡´ QLearnerì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤:
    - get_action(regime) -> (action_name, action_idx)
    - update(reward, next_regime, ...)
    - save() / load()
    
    ë‚´ë¶€ì ìœ¼ë¡œëŠ” ë‹¤ìŒì„ ì‚¬ìš©í•©ë‹ˆë‹¤:
    - StateEncoder: ì—°ì† ìƒíƒœ ì¸ì½”ë”©
    - DuelingDQN: ì‹ ê²½ë§ ê¸°ë°˜ Qê°’ ê³„ì‚°
    - ReplayBuffer: ê²½í—˜ ì¬í˜„
    - RewardShaper: ë‹¤ë©´ì  ë³´ìƒ ê³„ì‚°
    """
    
    def __init__(
        self,
        storage_path: Path,
        actions: Optional[List[str]] = None,
        model_name: str = "d3qn_model.pt",
    ):
        """
        Args:
            storage_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            actions: í–‰ë™ ê³µê°„ (ê¸°ë³¸: DEFAULT_ACTIONS)
            model_name: ëª¨ë¸ íŒŒì¼ ì´ë¦„
        """
        self.actions = actions or DEFAULT_ACTIONS
        self.n_actions = len(self.actions)
        self.storage_path = Path(storage_path)
        self.model_path = self.storage_path / model_name
        
        # ì¥ì¹˜ ì„¤ì •
        self.device = get_device()
        
        # ìƒíƒœ ì¸ì½”ë”
        self.state_encoder = get_state_encoder()
        self.state_dim = self.state_encoder.get_state_dim()
        
        # ì‹ ê²½ë§ (Online & Target)
        self.online_net, self.target_net = create_dqn_pair(
            state_dim=self.state_dim,
            n_actions=self.n_actions,
            device=self.device,
        )
        
        # ì˜µí‹°ë§ˆì´ì € (PyTorch ìˆì„ ë•Œë§Œ)
        if TORCH_AVAILABLE:
            self.optimizer = torch.optim.Adam(
                self.online_net.parameters(),
                lr=config.D3QN_LEARNING_RATE,
            )
        else:
            self.optimizer = None
        
        # ê²½í—˜ ì¬í˜„ ë²„í¼
        self.replay_buffer = create_replay_buffer(
            prioritized=False,  # ê¸°ë³¸ì€ Uniform
            capacity=config.D3QN_BUFFER_SIZE,
            batch_size=config.D3QN_BATCH_SIZE,
        )
        
        # ë³´ìƒ ê³„ì‚°ê¸°
        self.reward_shaper = get_reward_shaper()
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma = config.D3QN_GAMMA
        self.tau = config.D3QN_TAU
        self.epsilon = config.RL_EPSILON_START
        self.epsilon_decay = config.RL_EPSILON_DECAY
        self.epsilon_min = config.RL_EPSILON_MIN
        self.update_freq = config.D3QN_UPDATE_FREQ
        self.target_update_freq = config.D3QN_TARGET_UPDATE_FREQ
        
        # [V10] Epsilon Reheat ì„¤ì • - ì •ì±… ê³ ì°© ë°©ì§€
        self.reheat_enabled = config.RL_EPSILON_REHEAT_ENABLED
        self.reheat_period = config.RL_EPSILON_REHEAT_PERIOD
        self.reheat_value = config.RL_EPSILON_REHEAT_VALUE
        self.reheat_count = 0  # Reheat íšŸìˆ˜ ì¶”ì 
        
        # ìƒíƒœ ì¶”ì 
        self.last_state: Optional[np.ndarray] = None
        self.last_action_idx: Optional[int] = None
        self.step_count = 0
        self.learn_count = 0
        
        # í•™ìŠµ ë° ì„±ê³¼ í†µê³„
        self.losses: List[float] = []
        self.reward_history: List[float] = []
        self.best_reward_rolling: float = -999.0
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.load()
        
        logger.info(f"D3QN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”ë¨ - ìƒíƒœ: {self.state_dim}, í–‰ë™: {self.n_actions}, ì¥ì¹˜: {self.device}")
    
    def get_action(
        self,
        regime: RegimeState,
        df=None,  # pd.DataFrame, ì„ íƒì 
    ) -> Tuple[str, int]:
        """
        í˜„ì¬ ìƒíƒœì—ì„œ í–‰ë™ì„ ì„ íƒí•©ë‹ˆë‹¤ (epsilon-greedy).
        
        ê¸°ì¡´ QLearnerì™€ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        
        Args:
            regime: í˜„ì¬ ì‹œì¥ ìƒíƒœ
            df: ì›ì‹œ ë°ì´í„°í”„ë ˆì„ (ì„ íƒì , ë” ì •í™•í•œ ìƒíƒœ ì¸ì½”ë”©ìš©)
        
        Returns:
            (action_name, action_idx) íŠœí”Œ
        """
        # ìƒíƒœ ì¸ì½”ë”©
        if df is not None:
            encoded = self.state_encoder.encode(df)
            state = encoded.vector
        else:
            # DataFrame ì—†ìœ¼ë©´ RegimeStateì—ì„œ ì¶”ì¶œ
            state = self.state_encoder.encode_from_regime(regime)
        
        # Epsilon-greedy í–‰ë™ ì„ íƒ
        if random.random() < self.epsilon:
            action_idx = random.randint(0, self.n_actions - 1)
            exploration = True
        else:
            action_idx = self._get_best_action(state)
            exploration = False
        
        action_name = self.actions[action_idx]
        
        # ìƒíƒœ ì €ì¥ (updateì—ì„œ ì‚¬ìš©)
        self.last_state = state
        self.last_action_idx = action_idx
        
        # ë¡œê¹…
        if exploration:
            logger.debug(f"    [D3QN] íƒìƒ‰ (Îµ={self.epsilon:.3f}) -> {action_name}")
        else:
            logger.debug(f"    [D3QN] í™œìš© -> {action_name}")
        
        return action_name, action_idx
    
    def _get_best_action(self, state: np.ndarray) -> int:
        """ìµœì  í–‰ë™ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if TORCH_AVAILABLE:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                if self.device != "cpu":
                    state_tensor = state_tensor.to(self.device)
                q_values = self.online_net(state_tensor)
                return q_values.argmax(dim=-1).item()
        else:
            q_values = self.online_net(state)
            return int(np.argmax(q_values))
    
    def update(
        self,
        reward: float,
        next_regime: RegimeState,
        state_key: Optional[str] = None,  # í˜¸í™˜ì„±ìš©, ì‚¬ìš© ì•ˆ í•¨
        action_idx: Optional[int] = None,  # í˜¸í™˜ì„±ìš©
        df=None,  # pd.DataFrame, ì„ íƒì 
        metrics: Optional[Dict] = None,  # CPCV ì§€í‘œ (ë³´ìƒ ì¬ê³„ì‚°ìš©)
    ):
        """
        ê²½í—˜ì„ ì €ì¥í•˜ê³  ì‹ ê²½ë§ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        
        ê¸°ì¡´ QLearnerì™€ ë™ì¼í•œ ì‹œê·¸ë‹ˆì²˜ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        
        Args:
            reward: ë³´ìƒ (ë˜ëŠ” metricsì—ì„œ ê³„ì‚°)
            next_regime: ë‹¤ìŒ ì‹œì¥ ìƒíƒœ
            state_key: ì‚¬ìš© ì•ˆ í•¨ (í˜¸í™˜ì„±)
            action_idx: í–‰ë™ ì¸ë±ìŠ¤ (Noneì´ë©´ last_action_idx ì‚¬ìš©)
            df: ë‹¤ìŒ ìƒíƒœ ì¸ì½”ë”©ìš© ë°ì´í„°í”„ë ˆì„
            metrics: CPCV ì§€í‘œ (ë³´ìƒ ì¬ê³„ì‚°ìš©)
        """
        if self.last_state is None:
            logger.warning("ì´ì „ ìƒíƒœê°€ ì—†ì–´ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return
        
        action_idx = action_idx if action_idx is not None else self.last_action_idx
        if action_idx is None:
            return
        
        # ë‹¤ìŒ ìƒíƒœ ì¸ì½”ë”©
        if df is not None:
            next_encoded = self.state_encoder.encode(df)
            next_state = next_encoded.vector
        else:
            next_state = self.state_encoder.encode_from_regime(next_regime)
        
        # ë³´ìƒ ì¬ê³„ì‚° (metrics ìˆìœ¼ë©´)
        is_rejected = False
        if metrics is not None:
            reward_breakdown = self.reward_shaper.compute_breakdown(metrics)
            reward = reward_breakdown.total
            is_rejected = reward_breakdown.is_rejected
            
            # ì„±ê³¼ ì¶”ì  (ì •ì²´ ê°ì§€ìš©)
            self.reward_history.append(float(reward))
            if len(self.reward_history) > self.reheat_period * 3:
                self.reward_history.pop(0)
        else:
            self.reward_history.append(float(reward))
        
        # [V11.2] íƒˆë½(Rejection) ì²˜ë¦¬ - í•™ìŠµì„ ìŠ¤í‚µí•˜ê³  ì‹¶ì€ ê²½ìš°
        if is_rejected and getattr(config, 'RL_SKIP_LEARNING_ON_REJECTION', False):
            logger.info(f"    [D3QN] Strategy REJECTED. Skipping experience storage.")
            return

        # ê²½í—˜ ì €ì¥
        experience = Experience(
            state=self.last_state,
            action=action_idx,
            reward=float(reward),
            next_state=next_state,
            done=False,
        )
        self.replay_buffer.push(experience)
        
        self.step_count += 1
        
        # í•™ìŠµ (ì¼ì • ë¹ˆë„ë¡œ)
        if (
            self.step_count % self.update_freq == 0 
            and self.replay_buffer.can_sample()
        ):
            loss = self._learn()
            if loss is not None:
                self.losses.append(loss)
                self.learn_count += 1
                
                # Target ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
                if self.learn_count % self.target_update_freq == 0:
                    soft_update(self.online_net, self.target_net, self.tau)
                    logger.debug(f"    [D3QN] Target ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ë¨")
        
        # Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # [V11.2] ì •ì²´ ê°ì§€ ê¸°ë°˜ Epsilon Reheat
        if self.reheat_enabled and self.step_count > 0 and self.step_count % self.reheat_period == 0:
            if self._is_stagnated():
                old_epsilon = self.epsilon
                self.epsilon = max(self.epsilon, self.reheat_value)
                self.reheat_count += 1
                logger.warning(
                    f"    [D3QN] ğŸ”¥ STAGNATION DETECTED! Reheat #{self.reheat_count} | "
                    f"Îµ: {old_epsilon:.3f} â†’ {self.epsilon:.3f} | "
                    f"ì§€í‘œ ê°œì„  ì •ì²´ë¡œ ì¸í•œ íƒìƒ‰ ê°•ì œ ì¬ê°œ"
                )
            else:
                logger.info(f"    [D3QN] Performance improving (Top 10% Alpha), skipping reheat.")
        
        # ì£¼ê¸°ì  ì €ì¥
        if self.step_count % 100 == 0:
            self.save()
        
        logger.info(
            f"    [D3QN] ë³´ìƒ: {reward:.3f} | ë²„í¼: {len(self.replay_buffer)} | "
            f"Îµ: {self.epsilon:.3f} | í•™ìŠµ: {self.learn_count} | Reheat: {self.reheat_count}"
        )
    
    def _is_stagnated(self) -> bool:
        """ìµœê·¼ ì„±ê³¼ê°€ ì´ì „ ê¸°ê°„ ëŒ€ë¹„ ê°œì„ ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if len(self.reward_history) < self.reheat_period * 2:
            return False
            
        # ìµœê·¼ window vs ì´ì „ window ìƒìœ„ 10% ì„±ê³¼ ë¹„êµ
        window = self.reheat_period
        recent_rewards = self.reward_history[-window:]
        prev_rewards = self.reward_history[-2*window:-window]
        
        recent_top_10 = np.percentile(recent_rewards, 90)
        prev_top_10 = np.percentile(prev_rewards, 90)
        
        # ì´ì „ë³´ë‹¤ ìƒìœ„ê¶Œ ì ìˆ˜ê°€ ë‚®ê±°ë‚˜ ê±°ì˜ ì°¨ì´ê°€ ì—†ìœ¼ë©´(0.05 ë¯¸ë§Œ) ì •ì²´ë¡œ íŒë‹¨
        return recent_top_10 <= prev_top_10 + 0.05

    def _learn(self) -> Optional[float]:
        """
        ê²½í—˜ ì¬í˜„ ë²„í¼ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
        
        Double DQN Loss:
        - action_select = argmax(Q_online(s'))
        - Q_target = r + gamma * Q_target(s', action_select)
        - Loss = MSE(Q_online(s, a), Q_target)
        
        Returns:
            ì†ì‹¤ê°’ (float) ë˜ëŠ” None
        """
        if not TORCH_AVAILABLE:
            # NumPy ë²„ì „ - ê°„ë‹¨í•œ ì—…ë°ì´íŠ¸
            return self._learn_numpy()
        
        batch = self.replay_buffer.sample()
        
        # í…ì„œ ë³€í™˜
        states = torch.FloatTensor(batch.states).to(self.device)
        actions = torch.LongTensor(batch.actions).to(self.device)
        rewards = torch.FloatTensor(batch.rewards).to(self.device)
        next_states = torch.FloatTensor(batch.next_states).to(self.device)
        dones = torch.FloatTensor(batch.dones).to(self.device)
        
        # Double DQN: Onlineìœ¼ë¡œ í–‰ë™ ì„ íƒ, Targetìœ¼ë¡œ ê°€ì¹˜ í‰ê°€
        with torch.no_grad():
            # Online ë„¤íŠ¸ì›Œí¬ë¡œ ìµœì  í–‰ë™ ì„ íƒ
            next_actions = self.online_net(next_states).argmax(dim=-1)
            # Target ë„¤íŠ¸ì›Œí¬ë¡œ í•´ë‹¹ í–‰ë™ì˜ Qê°’ í‰ê°€
            next_q_values = self.target_net(next_states).gather(
                1, next_actions.unsqueeze(-1)
            ).squeeze(-1)
            # íƒ€ê²Ÿ ê³„ì‚°
            target_q = rewards + self.gamma * next_q_values * (1 - dones)
        
        # í˜„ì¬ Qê°’
        current_q = self.online_net(states).gather(
            1, actions.unsqueeze(-1)
        ).squeeze(-1)
        
        # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
        loss = F.mse_loss(current_q, target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), 1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def _learn_numpy(self) -> float:
        """NumPy ê¸°ë°˜ ê°„ë‹¨í•œ í•™ìŠµ (í´ë°±)."""
        batch = self.replay_buffer.sample()
        
        # ê°„ë‹¨í•œ Q-learning ì—…ë°ì´íŠ¸
        lr = config.D3QN_LEARNING_RATE
        
        total_loss = 0.0
        for i in range(len(batch.states)):
            state = batch.states[i]
            action = batch.actions[i]
            reward = batch.rewards[i]
            next_state = batch.next_states[i]
            done = batch.dones[i]
            
            # í˜„ì¬ Qê°’
            current_q = self.online_net(state)[0, action]
            
            # íƒ€ê²Ÿ Qê°’
            if done:
                target_q = reward
            else:
                next_q = self.target_net(next_state).max()
                target_q = reward + self.gamma * next_q
            
            # ì—…ë°ì´íŠ¸
            td_error = target_q - current_q
            self.online_net.W[:, action] += lr * td_error * state
            self.online_net.b[action] += lr * td_error
            
            total_loss += td_error ** 2
        
        return total_loss / len(batch.states)
    
    def save(self) -> None:
        """ëª¨ë¸ê³¼ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            self.storage_path.mkdir(parents=True, exist_ok=True)
            
            if TORCH_AVAILABLE:
                torch.save({
                    'online_state_dict': self.online_net.state_dict(),
                    'target_state_dict': self.target_net.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epsilon': self.epsilon,
                    'step_count': self.step_count,
                    'learn_count': self.learn_count,
                    'actions': self.actions,
                }, self.model_path)
            else:
                np.savez(
                    self.model_path.with_suffix('.npz'),
                    online_W=self.online_net.W,
                    online_b=self.online_net.b,
                    epsilon=self.epsilon,
                    step_count=self.step_count,
                )
            
            logger.debug(f"D3QN ëª¨ë¸ ì €ì¥ë¨: {self.model_path}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load(self) -> None:
        """ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.model_path.exists():
            try:
                if TORCH_AVAILABLE:
                    checkpoint = torch.load(self.model_path, map_location=self.device)
                    self.online_net.load_state_dict(checkpoint['online_state_dict'])
                    self.target_net.load_state_dict(checkpoint['target_state_dict'])
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    self.epsilon = checkpoint.get('epsilon', self.epsilon)
                    self.step_count = checkpoint.get('step_count', 0)
                    self.learn_count = checkpoint.get('learn_count', 0)
                else:
                    data = np.load(self.model_path.with_suffix('.npz'))
                    self.online_net.W = data['online_W']
                    self.online_net.b = data['online_b']
                    self.epsilon = float(data['epsilon'])
                    self.step_count = int(data['step_count'])
                logger.info(f"D3QN ëª¨ë¸ ë¡œë“œë¨: {self.model_path} (Îµ={self.epsilon:.3f})")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")


class IntegratedD3QNAgent(D3QNAgent):
    """
    [V11.4] Integrated Multi-head D3QN Agent.
    í•™ìŠµ íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•´ ì „ëµ(Strategy)ê³¼ ë¦¬ìŠ¤í¬(Risk Profile)ë¥¼ í•œ ì‹ ê²½ë§ì—ì„œ ë™ì‹œì— í•™ìŠµí•©ë‹ˆë‹¤.
    """
    def __init__(
        self,
        storage_path: Path,
        strategy_actions: List[str],
        risk_actions: List[str],
        model_name: str = "integrated_d3qn.pt"
    ):
        self.strategy_actions = strategy_actions
        self.risk_actions = risk_actions
        self.head_dims = [len(strategy_actions), len(risk_actions)]
        
        self.storage_path = Path(storage_path)
        self.model_path = self.storage_path / model_name
        
        self.device = get_device()
        self.state_encoder = get_state_encoder()
        self.state_dim = self.state_encoder.get_state_dim()
        
        if not TORCH_AVAILABLE:
            raise RuntimeError("IntegratedD3QNAgent requires PyTorch.")

        # Multi-head Networks
        from src.l3_meta.d3qn import MultiHeadDuelingDQN
        self.online_net = MultiHeadDuelingDQN(self.state_dim, self.head_dims).to(self.device)
        self.target_net = MultiHeadDuelingDQN(self.state_dim, self.head_dims).to(self.device)
        self.target_net.load_state_dict(self.online_net.state_dict())
        self.target_net.eval()
        
        self.optimizer = torch.optim.Adam(self.online_net.parameters(), lr=config.D3QN_LEARNING_RATE)
        self.replay_buffer = create_replay_buffer(multi_action=True)
        self.reward_shaper = get_reward_shaper()
        
        # RL Hyperparams
        self.epsilon = config.D3QN_EPSILON
        self.gamma = config.D3QN_GAMMA
        self.tau = config.D3QN_TAU
        
        # Monitoring
        self.step_count = 0
        self.learn_count = 0
        self.reheat_count = 0
        self.reward_history = []
        self.last_experience = None # (state, [strategy_idx, risk_idx])

        # Stagnation
        self.reheat_period = getattr(config, "D3QN_REHEAT_PERIOD", 100)
        self.reheat_value = getattr(config, "D3QN_REHEAT_EPSILON", 0.3)
        
        self.load()

    def get_action(self, regime: RegimeState, df: Optional[pd.DataFrame] = None) -> Tuple[str, int, str, int]:
        """
        ì „ëµê³¼ ë¦¬ìŠ¤í¬ í–‰ë™ì„ ë™ì‹œì— ì„ íƒí•©ë‹ˆë‹¤.
        Returns: (strat_name, strat_idx, risk_name, risk_idx)
        """
        if df is not None:
            state = self.state_encoder.encode(df).vector
        else:
            state = self.state_encoder.encode_from_regime(regime)
            
        if random.random() < self.epsilon:
            s_idx = random.randint(0, self.head_dims[0] - 1)
            r_idx = random.randint(0, self.head_dims[1] - 1)
        else:
            with torch.no_grad():
                st = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_heads = self.online_net(st)
                s_idx = q_heads[0].argmax(dim=-1).item()
                r_idx = q_heads[1].argmax(dim=-1).item()
        
        self.last_experience = (state, [s_idx, r_idx])
        return self.strategy_actions[s_idx], s_idx, self.risk_actions[r_idx], r_idx

    def update(self, reward: float, next_regime: RegimeState, next_df: Optional[pd.DataFrame] = None):
        if self.last_experience is None: return
        
        state, actions = self.last_experience
        self.reward_history.append(reward)
        self.step_count += 1
        
        if next_df is not None:
            next_state = self.state_encoder.encode(next_df).vector
        else:
            next_state = self.state_encoder.encode_from_regime(next_regime)
            
        self.replay_buffer.push_transition(state, actions, reward, next_state)
        self.last_experience = None

        if self.replay_buffer.can_sample():
            loss = self._learn()
            if loss is not None:
                self.learn_count += 1
                if self.learn_count % 10 == 0:
                    soft_update(self.online_net, self.target_net, self.tau)

        # Decay epsilon
        self.epsilon = max(config.D3QN_EPSILON_MIN, self.epsilon * config.D3QN_EPSILON_DECAY)
        
        # Reheat logic
        if self.step_count > 0 and self.step_count % self.reheat_period == 0:
            if self._is_stagnated():
                self.epsilon = max(self.epsilon, self.reheat_value)
                self.reheat_count += 1
                logger.warning(f"[IntegratedD3QN] ğŸ”¥ Stagnation detected. Reheat #{self.reheat_count} Îµ={self.epsilon:.2f}")

        if self.step_count % 100 == 0:
            self.save()
            
        self.step_count += 1

    def _learn(self) -> Optional[float]:
        batch = self.replay_buffer.sample()
        if batch is None: return None
        
        states = torch.FloatTensor(batch.states).to(self.device)
        rewards = torch.FloatTensor(batch.rewards).to(self.device)
        next_states = torch.FloatTensor(batch.next_states).to(self.device)
        dones = torch.FloatTensor(batch.dones).to(self.device)
        
        self.optimizer.zero_grad()
        
        # Forward pass
        with torch.no_grad():
            next_q_heads_online = self.online_net(next_states)
            next_q_heads_target = self.target_net(next_states)
            
        current_q_heads = self.online_net(states)
        
        total_loss = 0
        for i in range(len(self.head_dims)):
            head_actions = torch.LongTensor(batch.actions_list[i]).to(self.device)
            # Double DQN: argmax from online, value from target
            best_next_actions = next_q_heads_online[i].argmax(dim=-1)
            next_q = next_q_heads_target[i].gather(1, best_next_actions.unsqueeze(-1)).squeeze(-1)
            target_q = rewards + self.gamma * next_q * (1 - dones)
            
            # Current Q
            curr_q = current_q_heads[i].gather(1, head_actions.unsqueeze(-1)).squeeze(-1)
            total_loss += F.mse_loss(curr_q, target_q)
            
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.online_net.parameters(), 1.0)
        self.optimizer.step()
        
        return total_loss.item()

    def save(self):
        self.storage_path.mkdir(parents=True, exist_ok=True)
        torch.save({
            'online_state_dict': self.online_net.state_dict(),
            'target_state_dict': self.target_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count,
            'learn_count': self.learn_count,
            'reheat_count': self.reheat_count,
            'strategy_actions': self.strategy_actions,
            'risk_actions': self.risk_actions,
        }, self.model_path)

    def load(self):
        if self.model_path.exists():
            try:
                ckpt = torch.load(self.model_path, map_location=self.device)
                self.online_net.load_state_dict(ckpt['online_state_dict'])
                self.target_net.load_state_dict(ckpt['target_net_state_dict'] if 'target_net_state_dict' in ckpt else ckpt['target_state_dict'])
                if 'optimizer_state_dict' in ckpt:
                    self.optimizer.load_state_dict(ckpt['optimizer_state_dict'])
                self.epsilon = ckpt.get('epsilon', self.epsilon)
                self.step_count = ckpt.get('step_count', 0)
                self.learn_count = ckpt.get('learn_count', 0)
                self.reheat_count = ckpt.get('reheat_count', 0)
                logger.info(f"Integrated D3QN loaded: {self.model_path}")
            except Exception as e:
                logger.error(f"Load failed: {e}")


def get_integrated_agent(
    storage_path: Path,
    strategy_actions: List[str],
    risk_actions: List[str]
) -> IntegratedD3QNAgent:
    """Integrated ì—ì´ì „íŠ¸ íŒ©í† ë¦¬ í•¨ìˆ˜."""
    return IntegratedD3QNAgent(storage_path, strategy_actions, risk_actions)


# QLearnerì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ íŒ©í† ë¦¬ í•¨ìˆ˜
def create_rl_agent(
    storage_path: Path,
    actions: Optional[List[str]] = None,
    use_deep_rl: bool = None,
) -> 'D3QNAgent':
    """
    RL ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        storage_path: ì €ì¥ ê²½ë¡œ
        actions: í–‰ë™ ê³µê°„
        use_deep_rl: Trueë©´ D3QN, Falseë©´ ê¸°ì¡´ QLearner (ê¸°ë³¸: config.D3QN_ENABLED)
    
    Returns:
        D3QNAgent ë˜ëŠ” QLearner ì¸ìŠ¤í„´ìŠ¤
    """
    use_deep = use_deep_rl if use_deep_rl is not None else config.D3QN_ENABLED
    
    if use_deep:
        return D3QNAgent(storage_path, actions)
    else:
        # ê¸°ì¡´ QLearner ì„í¬íŠ¸ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from src.l3_meta.q_learner import QLearner
        return QLearner(storage_path, actions)
